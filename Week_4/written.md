## Question
### The Problems of Stochasticity (SGD)

SGD's stochasticity is believed to help escape shallow local minima. However, could this unstable gradient update also hinder the model's convergence to a deeper, better minimum in later stages of training, causing it to "judder" around the optimal solution? Is learning rate decay the only solution to this problem?